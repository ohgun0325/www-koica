"""FastAPI server for LangChain chatbot with pgvector."""

import asyncio
from contextlib import asynccontextmanager
from typing import Optional, Union

import psycopg2
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse

from app.router import chat_router
from app.routes import health_router, search_router
from app.core import (
    insert_sample_data,
    setup_pgvector,
    wait_for_db,
)
from app.models.base import BaseLLMModel
from app.models.manager import ModelManager
from config import settings

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    ChatGoogleGenerativeAI = None  # type: ignore


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize database connection and chat model on startup."""
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")

    # Wait for database (ë™ê¸° ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
    await asyncio.to_thread(wait_for_db)

    # Setup pgvector (ë™ê¸° ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
    db_connection, embedding_dimension = await asyncio.to_thread(setup_pgvector)
    print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ (ì„ë² ë”© ì°¨ì›: {embedding_dimension})")

    # Insert sample data if table is empty (ë™ê¸° ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
    def check_and_insert_data():
        cur = db_connection.cursor()
        cur.execute("SELECT COUNT(*) FROM langchain_documents")
        count = cur.fetchone()[0]

        if count == 0:
            print("ğŸ“š ìƒ˜í”Œ ë°ì´í„° ì‚½ì… ì¤‘...")
            insert_sample_data(db_connection, embedding_dimension)
        else:
            print(f"âœ… ê¸°ì¡´ ë¬¸ì„œ {count}ê°œ ë°œê²¬")

    await asyncio.to_thread(check_and_insert_data)

    # Initialize QLoRA chat service or fallback to original model
    qlora_service = None
    chat_model: Optional[Union[BaseLLMModel, ChatGoogleGenerativeAI]] = None

    if settings.use_qlora:
        # QLoRA ëª¨ë¸ ì‚¬ìš©
        try:
            from app.service.chat_service import QLoRAChatService

            print("ğŸ“¦ QLoRA ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

            def init_qlora_service():
                service = QLoRAChatService(
                    model_name=settings.qlora_model_name,
                    output_dir=settings.qlora_output_dir,
                    use_4bit=settings.qlora_use_4bit,
                    bnb_4bit_compute_dtype=settings.qlora_bnb_4bit_compute_dtype,
                    bnb_4bit_quant_type=settings.qlora_bnb_4bit_quant_type,
                    bnb_4bit_use_double_quant=settings.qlora_bnb_4bit_use_double_quant,
                    device_map=settings.qlora_device_map,
                )
                service.load_model()
                return service

            # íƒ€ì„ì•„ì›ƒ ì„¤ì • (5ë¶„)
            try:
                qlora_service = await asyncio.wait_for(
                    asyncio.to_thread(init_qlora_service),
                    timeout=300.0
                )
                print("âœ… QLoRA ì±„íŒ… ì„œë¹„ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
            except asyncio.TimeoutError:
                print("âš ï¸  QLoRA ì„œë¹„ìŠ¤ ë¡œë“œ íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)")
                print("   Fallback: ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì‹œë„...")
                settings.use_qlora = False
                qlora_service = None
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìê°€ QLoRA ë¡œë”©ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            print("   Fallback: ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì‹œë„...")
            settings.use_qlora = False
            qlora_service = None
        except Exception as e:
            print(f"âš ï¸  QLoRA ì„œë¹„ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print("   Fallback: ê¸°ì¡´ ëª¨ë¸ ë¡œë”© ì‹œë„...")
            settings.use_qlora = False
            qlora_service = None

    if not settings.use_qlora or qlora_service is None:
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë”© (QLoRAê°€ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨í•œ ê²½ìš°)
        # Try to load Midm model first (ë™ê¸° ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
        if settings.default_chat_model:
            try:
                def load_midm_model():
                    manager = ModelManager()
                    return manager.get_chat_model(settings.default_chat_model)

                chat_model = await asyncio.to_thread(load_midm_model)
                if chat_model:
                    print(f"âœ… Midm ëª¨ë¸ '{settings.default_chat_model}' ë¡œë“œ ì™„ë£Œ!")
                else:
                    print(f"âš ï¸  Midm ëª¨ë¸ '{settings.default_chat_model}' ë¡œë“œ ì‹¤íŒ¨")
            except Exception as e2:
                print(f"âš ï¸  Midm ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e2)[:100]}")

        # Fallback to Gemini API if Midm is not available
        print(f"ğŸ” Gemini API ë¡œë“œ ì‹œë„ - gemini_api_key ì„¤ì • ì—¬ë¶€: {settings.gemini_api_key is not None}")
        if chat_model is None and settings.gemini_api_key:
            print(f"ğŸ”‘ Gemini API í‚¤ ê¸¸ì´: {len(settings.gemini_api_key)} (ì²˜ìŒ 10ì: {settings.gemini_api_key[:10]}...)")
            try:
                def load_gemini_model():
                    from app.core import get_chat_model
                    return get_chat_model()

                gemini_model = await asyncio.to_thread(load_gemini_model)
                if gemini_model:
                    chat_model = gemini_model
                    print("âœ… Gemini API ì—°ê²° í™•ì¸ ì™„ë£Œ!")
                else:
                    print("âš ï¸  Gemini APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (get_chat_model()ì´ None ë°˜í™˜)")
            except Exception as e2:
                print(f"âš ï¸  Gemini API ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e2)[:200]}")
                import traceback
                traceback.print_exc()
        elif chat_model is None:
            print(f"âš ï¸  Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. settings.gemini_api_key = {settings.gemini_api_key}")

        if chat_model is None:
            print("âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ ì±„íŒ… ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        app.state.chat_model = chat_model
    else:
        # QLoRAë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” chat_modelì„ Noneìœ¼ë¡œ ì„¤ì • (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
        app.state.chat_model = None

    # Store in app state
    app.state.db_connection = db_connection
    app.state.embedding_dimension = embedding_dimension
    app.state.qlora_service = qlora_service

    print("âœ… FastAPI ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

    yield

    # Cleanup on shutdown (ë™ê¸° ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
    qlora_service = getattr(app.state, "qlora_service", None)
    if qlora_service:
        await asyncio.to_thread(qlora_service.unload_model)
        print("ğŸ‘‹ QLoRA ì„œë¹„ìŠ¤ ì–¸ë¡œë“œ ì™„ë£Œ")

    if db_connection:
        await asyncio.to_thread(db_connection.close)
        print("ğŸ‘‹ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")


# Create FastAPI app
app = FastAPI(
    title=settings.app_title,
    description=settings.app_description,
    version=settings.app_version,
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=settings.cors_allow_credentials,
    allow_methods=settings.cors_allow_methods,
    allow_headers=settings.cors_allow_headers,
)

# Include routers
app.include_router(health_router)
app.include_router(chat_router)
app.include_router(search_router)


@app.get("/", response_class=HTMLResponse)
async def read_root():
    """Serve the main HTML page."""
    try:
        with open("static/index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return """
        <html>
            <head><title>LangChain Chatbot</title></head>
            <body>
                <h1>LangChain Chatbot API</h1>
                <p>API ë¬¸ì„œ: <a href="/docs">/docs</a></p>
                <p>í”„ë¡ íŠ¸ì—”ë“œ: <a href="http://localhost:3000">Next.js í”„ë¡ íŠ¸ì—”ë“œ</a></p>
            </body>
        </html>
        """


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
    )

