"""RAG chat chain implementation."""

from typing import Optional, Union

import psycopg2

from app.core.embeddings import generate_embeddings
from app.core.vectorstore import query_similar_documents
from app.models.base import BaseLLMModel

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError:
    ChatGoogleGenerativeAI = None  # type: ignore
    HumanMessage = None  # type: ignore
    SystemMessage = None  # type: ignore


def chat_with_ai(
    conn: psycopg2.extensions.connection,
    user_input: str,
    dimension: int,
    chat_model: Optional[Union[BaseLLMModel, ChatGoogleGenerativeAI]] = None
) -> str:
    """Chat with AI using RAG (Retrieval Augmented Generation).

    Args:
        conn: Database connection object.
        user_input: User's question or message.
        dimension: Expected embedding dimension.
        chat_model: Chat model instance (BaseLLMModel or ChatGoogleGenerativeAI).

    Returns:
        AI's response as a string.
    """
    # Generate embedding for user query
    print("ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    query_embeddings = generate_embeddings([user_input], dimension)
    query_vector = query_embeddings[0]

    # Search for similar documents
    similar_docs = query_similar_documents(conn, query_vector, limit=3)

    # If chat model is not available, return search results only
    if chat_model is None:
        if similar_docs:
            context = "\n\n".join([content for _, content, _ in similar_docs])
            return f"""âš ï¸ ì±„íŒ… ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:

{context}

ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
        else:
            return "âš ï¸ ì±„íŒ… ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê³ , ê´€ë ¨ ë¬¸ì„œë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # Check if it's a BaseLLMModel (Midm) or Gemini model
    is_base_model = isinstance(chat_model, BaseLLMModel)

    # Prepare messages
    if is_base_model:
        # BaseLLMModel (Midm) - use list format with improved prompt
        # Format context more naturally (remove bullet points)
        formatted_context = "\n\n".join([content for _, content, _ in similar_docs])

        # Create a concise, natural system prompt for Midm
        system_content = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í•˜ë“¯ì´ ë‹µë³€í•´ì£¼ì„¸ìš”. ì •ë³´ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³  ìì‹ ì˜ ë§ë¡œ ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

ì°¸ê³  ì •ë³´:
{formatted_context}"""

        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_input}
        ]
    else:
        # Gemini model - use LangChain messages with improved prompt
        if SystemMessage is None or HumanMessage is None:
            return "âŒ LangChain ë©”ì‹œì§€ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # Format context more naturally (remove bullet points)
        formatted_context = "\n\n".join([content for _, content, _ in similar_docs])

        messages = [
            SystemMessage(content=f"""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì°¸ê³  ì •ë³´ì…ë‹ˆë‹¤:

{formatted_context}

**ì¤‘ìš” ì§€ì‹œì‚¬í•­:**
1. ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”í•˜ë“¯ì´ ë‹µë³€í•˜ì„¸ìš”.
2. ì •ë³´ë¥¼ ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ë¡œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ìì‹ ì˜ ë§ë¡œ ì‰½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
3. ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í†¤ìœ¼ë¡œ ëŒ€í™”í•˜ë“¯ì´ ë‹µë³€í•˜ì„¸ìš”.
4. ì°¸ê³  ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì´ í•„ìš”í•˜ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
5. "ì°¸ê³  ë¬¸ì„œ", "ì°¸ê³  ì •ë³´" ê°™ì€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ìì—°ìŠ¤ëŸ½ê²Œ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."""),
            HumanMessage(content=user_input)
        ]

    # Get AI response with error handling
    print("ğŸ¤– AIê°€ ì‘ë‹µ ìƒì„± ì¤‘...")
    try:
        response = chat_model.invoke(messages)

        # Handle different response formats
        if is_base_model:
            # BaseLLMModel returns string directly
            return str(response)
        else:
            # Gemini returns object with .content attribute
            return response.content if hasattr(response, 'content') else str(response)

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ ëª¨ë¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {error_msg[:200]}")

        # Fallback to search results
        if similar_docs:
            context = "\n\n".join([content for _, content, _ in similar_docs])
            return f"""âš ï¸ ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:

{context}

ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
        else:
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {error_msg[:200]}"

