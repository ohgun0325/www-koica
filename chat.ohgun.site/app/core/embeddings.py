"""Embedding generation utilities."""

from typing import List

from config import settings


def get_embedding_dimension() -> int:
    """Get the embedding dimension.

    Returns:
        Embedding dimension (768 for Midm or fallback).
    """
    # Midm ëª¨ë¸ ì‚¬ìš© ì‹œ ê¸°ë³¸ ì„ë² ë”© ì°¨ì›
    if settings.default_chat_model and settings.default_chat_model.lower() == "midm":
        return 768  # ì¼ë°˜ì ì¸ LLM ì„ë² ë”© ì°¨ì›

    # Gemini API í™•ì¸ (fallback)
    try:
        from app.core.gemini import test_gemini_api
        dim = test_gemini_api()
        if dim > 0:
            return dim
    except Exception:
        pass

    return 768  # ê¸°ë³¸ê°’ (Midm ì‚¬ìš© ì‹œ)


def generate_dummy_embeddings(count: int, dimension: int) -> List[List[float]]:
    """Generate dummy embeddings with specified dimension.

    Args:
        count: Number of embeddings to generate.
        dimension: Dimension of each embedding.

    Returns:
        List of dummy embedding vectors.
    """
    embeddings = []
    for i in range(count):
        # Create a simple pattern: first few dimensions are 1.0, rest are 0.0
        embedding = [0.0] * dimension
        if dimension > 0:
            embedding[i % dimension] = 1.0
        embeddings.append(embedding)
    return embeddings


def generate_embeddings(texts: List[str], dimension: int = 768) -> List[List[float]]:
    """Generate embeddings using Midm model or fallback methods.

    Args:
        texts: List of text strings to embed.
        dimension: Expected dimension for embeddings.

    Returns:
        List of embedding vectors.
    """
    # Midm ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„± ì‹œë„
    if settings.default_chat_model and settings.default_chat_model.lower() == "midm":
        try:
            from app.models.manager import ModelManager
            manager = ModelManager()
            chat_model = manager.get_chat_model("midm")

            if chat_model and hasattr(chat_model, '_tokenizer') and chat_model._tokenizer:
                embeddings = []
                for text in texts:
                    # Midm ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
                    # ì‹¤ì œë¡œëŠ” ëª¨ë¸ì˜ hidden statesë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì§€ë§Œ,
                    # ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ì„ë² ë”© ì‚¬ìš©
                    try:
                        import torch
                        import numpy as np

                        # í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
                        inputs = chat_model._tokenizer(
                            text,
                            return_tensors="pt",
                            padding=True,
                            truncation=True,
                            max_length=512
                        )

                        # ê°„ë‹¨í•œ ì„ë² ë”©: í† í° IDì˜ í‰ê· ì„ ì‚¬ìš©
                        # ì‹¤ì œë¡œëŠ” ëª¨ë¸ì˜ hidden statesë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•¨
                        token_ids = inputs['input_ids']

                        # ëª¨ë¸ì˜ hidden statesë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ ì„ë² ë”© ìƒì„±
                        if hasattr(chat_model, '_model') and chat_model._model and chat_model._model is not None:
                            with torch.no_grad():
                                # ëª¨ë¸ì„ í†µí•´ forward passí•˜ì—¬ hidden states ì–»ê¸°
                                try:
                                    outputs = chat_model._model(**inputs, output_hidden_states=True)
                                    # ë§ˆì§€ë§‰ hidden stateì˜ í‰ê· ì„ ì‚¬ìš© (ë¬¸ì¥ ì„ë² ë”©)
                                    hidden_states = outputs.hidden_states[-1]  # ë§ˆì§€ë§‰ ë ˆì´ì–´
                                    text_embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()
                                except Exception:
                                    # Fallback: ì…ë ¥ ì„ë² ë”© ë ˆì´ì–´ ì‚¬ìš©
                                    try:
                                        if hasattr(chat_model._model, 'get_input_embeddings'):
                                            embed_layer = chat_model._model.get_input_embeddings()
                                            token_embeddings = embed_layer(token_ids)
                                            text_embedding = token_embeddings.mean(dim=1).squeeze().cpu().numpy()
                                        else:
                                            raise Exception("No embedding method available")
                                    except Exception:
                                        # ìµœì¢… Fallback: ê°„ë‹¨í•œ í†µê³„ ê¸°ë°˜ ì„ë² ë”©
                                        text_embedding = np.random.normal(0, 0.1, dimension).astype(np.float32)
                        else:
                            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ í†µê³„ ê¸°ë°˜ ì„ë² ë”©
                            text_embedding = np.random.normal(0, 0.1, dimension).astype(np.float32)

                        # ì°¨ì› ë§ì¶”ê¸°
                        if len(text_embedding) != dimension:
                            if len(text_embedding) > dimension:
                                text_embedding = text_embedding[:dimension]
                            else:
                                padding = np.zeros(dimension - len(text_embedding))
                                text_embedding = np.concatenate([text_embedding, padding])

                        embeddings.append(text_embedding.tolist())
                    except Exception as e:
                        print(f"âš ï¸  Midm ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(e)[:100]}")
                        # ì˜¤ë¥˜ ì‹œ ë”ë¯¸ ì„ë² ë”© ì‚¬ìš©
                        embeddings.append(generate_dummy_embeddings(1, dimension)[0])

                if embeddings:
                    print(f"âœ… Midm ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ! (ì°¨ì›: {dimension})")
                    return embeddings
        except Exception as e:
            print(f"âš ï¸  Midm ëª¨ë¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}")
            print(f"   ëŒ€ì²´ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # Fallback: Gemini API ì‚¬ìš© (ì„ íƒì‚¬í•­)
    try:
        from app.core.gemini import get_embeddings_model
        embeddings_model = get_embeddings_model()
        if embeddings_model:
            try:
                embeddings = embeddings_model.embed_documents(texts)
                print("ğŸ¤– Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
                return embeddings
            except Exception as e:
                error_msg = str(e)
                print(f"âš ï¸  Gemini ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {error_msg[:100]}")
    except Exception:
        pass

    # ìµœì¢… Fallback: ë”ë¯¸ ì„ë² ë”©
    print(f"âš ï¸  ë”ë¯¸ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì°¨ì›: {dimension})")
    return generate_dummy_embeddings(len(texts), dimension)

