# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ë¥˜ - 2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (TF-IDF + Logistic Regression)
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import time
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("="*70)
print("ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ë¥˜ - ëª¨ë¸ í•™ìŠµ")
print("="*70)

# ============================================================
# 1. ë°ì´í„° ë¡œë“œ
# ============================================================
print("\n[1ë‹¨ê³„] ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ")

try:
    train_df = pd.read_csv('ratings_train_clean.csv')
    test_df = pd.read_csv('ratings_test_clean.csv')
    print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_df):,}ê°œ")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ")
except FileNotFoundError:
    print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    print("ë¨¼ì € 01_eda.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit()

# ============================================================
# 2. ë°ì´í„° ì¤€ë¹„
# ============================================================
print("\n[2ë‹¨ê³„] ë°ì´í„° ì¤€ë¹„")

X_train = train_df['document_clean'].values
y_train = train_df['label'].values

X_test = test_df['document_clean'].values
y_test = test_df['label'].values

print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}, ë ˆì´ë¸”: {y_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}, ë ˆì´ë¸”: {y_test.shape}")

# ============================================================
# 3. TF-IDF ë²¡í„°í™”
# ============================================================
print("\n[3ë‹¨ê³„] TF-IDF ë²¡í„°í™”")
print("â³ ë²¡í„°í™” ì§„í–‰ ì¤‘... (ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

start_time = time.time()

# TF-IDF Vectorizer ì„¤ì •
vectorizer = TfidfVectorizer(
    max_features=10000,      # ìƒìœ„ 10,000ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©
    min_df=2,                # ìµœì†Œ 2ê°œ ë¬¸ì„œì— ë“±ì¥í•´ì•¼ í•¨
    max_df=0.8,              # 80% ì´ìƒì˜ ë¬¸ì„œì— ë“±ì¥í•˜ë©´ ì œì™¸ (ë¶ˆìš©ì–´ íš¨ê³¼)
    ngram_range=(1, 2),      # 1-gram, 2-gram ëª¨ë‘ ì‚¬ìš©
    sublinear_tf=True        # TFì— ë¡œê·¸ ìŠ¤ì¼€ì¼ ì ìš©
)

# í•™ìŠµ ë°ì´í„°ë¡œ fit & transform
X_train_tfidf = vectorizer.fit_transform(X_train)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” transformë§Œ
X_test_tfidf = vectorizer.transform(X_test)

elapsed_time = time.time() - start_time

print(f"âœ… ë²¡í„°í™” ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
print(f"í•™ìŠµ ë°ì´í„° shape: {X_train_tfidf.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {X_test_tfidf.shape}")
print(f"ì‚¬ìš©ëœ ë‹¨ì–´(íŠ¹ì„±) ê°œìˆ˜: {len(vectorizer.get_feature_names_out()):,}ê°œ")

# TF-IDF ë²¡í„°ë¼ì´ì € ì €ì¥
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
print("âœ… TF-IDF ë²¡í„°ë¼ì´ì € ì €ì¥: tfidf_vectorizer.pkl")

# ============================================================
# 4. ëª¨ë¸ í•™ìŠµ
# ============================================================
print("\n[4ë‹¨ê³„] Logistic Regression ëª¨ë¸ í•™ìŠµ")
print("â³ í•™ìŠµ ì§„í–‰ ì¤‘...")

start_time = time.time()

# Logistic Regression ëª¨ë¸
model = LogisticRegression(
    max_iter=1000,           # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
    C=1.0,                   # ì •ê·œí™” ê°•ë„ (ì‘ì„ìˆ˜ë¡ ê°•í•¨)
    random_state=42,
    solver='lbfgs',          # ìµœì í™” ì•Œê³ ë¦¬ì¦˜
    n_jobs=-1                # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
)

# í•™ìŠµ
model.fit(X_train_tfidf, y_train)

elapsed_time = time.time() - start_time

print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

# ëª¨ë¸ ì €ì¥
joblib.dump(model, 'sentiment_model.pkl')
print("âœ… ëª¨ë¸ ì €ì¥: sentiment_model.pkl")

# ============================================================
# 5. êµì°¨ ê²€ì¦
# ============================================================
print("\n[5ë‹¨ê³„] êµì°¨ ê²€ì¦ (5-Fold)")
print("â³ ê²€ì¦ ì§„í–‰ ì¤‘... (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

# ìƒ˜í”Œë§í•´ì„œ ë¹ ë¥´ê²Œ êµì°¨ ê²€ì¦ (ì „ì²´ ë°ì´í„°ë¡œ í•˜ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
sample_size = 20000
sample_indices = np.random.choice(len(X_train), sample_size, replace=False)
X_train_sample = X_train_tfidf[sample_indices]
y_train_sample = y_train[sample_indices]

start_time = time.time()
cv_scores = cross_val_score(model, X_train_sample, y_train_sample, cv=5, scoring='accuracy', n_jobs=-1)
elapsed_time = time.time() - start_time

print(f"âœ… êµì°¨ ê²€ì¦ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
print(f"CV ì ìˆ˜: {cv_scores}")
print(f"í‰ê·  ì •í™•ë„: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# ============================================================
# 6. í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€
# ============================================================
print("\n[6ë‹¨ê³„] í•™ìŠµ ë°ì´í„° í‰ê°€")

y_train_pred = model.predict(X_train_tfidf)
train_accuracy = accuracy_score(y_train, y_train_pred)

print(f"í•™ìŠµ ë°ì´í„° ì •í™•ë„: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")

# ============================================================
# 7. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€
# ============================================================
print("\n[7ë‹¨ê³„] í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€")

y_test_pred = model.predict(X_test_tfidf)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\nğŸ“Š ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_test_pred, 
                          target_names=['ë¶€ì •(0)', 'ê¸ì •(1)'],
                          digits=4))

# ============================================================
# 8. Confusion Matrix ì‹œê°í™”
# ============================================================
print("\n[8ë‹¨ê³„] Confusion Matrix ìƒì„±")

cm = confusion_matrix(y_test, y_test_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['ë¶€ì •(0)', 'ê¸ì •(1)'],
            yticklabels=['ë¶€ì •(0)', 'ê¸ì •(1)'],
            cbar_kws={'label': 'ê°œìˆ˜'})
plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
plt.ylabel('ì‹¤ì œ ë ˆì´ë¸”', fontsize=12)
plt.xlabel('ì˜ˆì¸¡ ë ˆì´ë¸”', fontsize=12)

# ì •í™•ë„ í‘œì‹œ
for i in range(2):
    for j in range(2):
        value = cm[i, j]
        total = cm[i].sum()
        percentage = value / total * 100
        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', 
                ha='center', va='center', fontsize=10, color='gray')

plt.tight_layout()
plt.savefig('03_confusion_matrix.png', dpi=300, bbox_inches='tight')
print("âœ… Confusion Matrix ì €ì¥: 03_confusion_matrix.png")
plt.close()

# ============================================================
# 9. ì£¼ìš” íŠ¹ì„±(ë‹¨ì–´) ë¶„ì„
# ============================================================
print("\n[9ë‹¨ê³„] ì£¼ìš” íŠ¹ì„±(ë‹¨ì–´) ë¶„ì„")

# ëª¨ë¸ì˜ ê³„ìˆ˜ ì¶”ì¶œ
feature_names = vectorizer.get_feature_names_out()
coefficients = model.coef_[0]

# ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë‹¨ì–´ (ê³„ìˆ˜ê°€ í° ë‹¨ì–´)
top_positive_idx = np.argsort(coefficients)[-20:][::-1]
top_positive_words = [(feature_names[i], coefficients[i]) for i in top_positive_idx]

# ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë‹¨ì–´ (ê³„ìˆ˜ê°€ ì‘ì€ ë‹¨ì–´)
top_negative_idx = np.argsort(coefficients)[:20]
top_negative_words = [(feature_names[i], coefficients[i]) for i in top_negative_idx]

print("\nâœ¨ ê¸ì • ì˜ˆì¸¡ì— ê°€ì¥ ì˜í–¥ì„ ì£¼ëŠ” ë‹¨ì–´ TOP 20:")
print("="*70)
for i, (word, coef) in enumerate(top_positive_words, 1):
    print(f"{i:2d}. {word:20s} (ê³„ìˆ˜: {coef:8.4f})")

print("\nğŸ’” ë¶€ì • ì˜ˆì¸¡ì— ê°€ì¥ ì˜í–¥ì„ ì£¼ëŠ” ë‹¨ì–´ TOP 20:")
print("="*70)
for i, (word, coef) in enumerate(top_negative_words, 1):
    print(f"{i:2d}. {word:20s} (ê³„ìˆ˜: {coef:8.4f})")

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# ê¸ì • ë‹¨ì–´
pos_words = [w for w, _ in top_positive_words]
pos_coefs = [c for _, c in top_positive_words]
axes[0].barh(range(len(pos_words)), pos_coefs, color='#4ecdc4')
axes[0].set_yticks(range(len(pos_words)))
axes[0].set_yticklabels(pos_words)
axes[0].set_xlabel('ê³„ìˆ˜ ê°’', fontsize=11)
axes[0].set_title('ê¸ì • ì˜ˆì¸¡ì— ì˜í–¥ì„ ì£¼ëŠ” ë‹¨ì–´ TOP 20', fontsize=12, fontweight='bold')
axes[0].invert_yaxis()

# ë¶€ì • ë‹¨ì–´
neg_words = [w for w, _ in top_negative_words]
neg_coefs = [c for _, c in top_negative_words]
axes[1].barh(range(len(neg_words)), neg_coefs, color='#ff6b6b')
axes[1].set_yticks(range(len(neg_words)))
axes[1].set_yticklabels(neg_words)
axes[1].set_xlabel('ê³„ìˆ˜ ê°’', fontsize=11)
axes[1].set_title('ë¶€ì • ì˜ˆì¸¡ì— ì˜í–¥ì„ ì£¼ëŠ” ë‹¨ì–´ TOP 20', fontsize=12, fontweight='bold')
axes[1].invert_yaxis()

plt.tight_layout()
plt.savefig('04_feature_importance.png', dpi=300, bbox_inches='tight')
print("\nâœ… íŠ¹ì„± ì¤‘ìš”ë„ ê·¸ë˜í”„ ì €ì¥: 04_feature_importance.png")
plt.close()

# ============================================================
# 10. ìµœì¢… ìš”ì•½
# ============================================================
print("\n" + "="*70)
print("ğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
print("="*70)

print(f"\nâœ… ëª¨ë¸: Logistic Regression")
print(f"âœ… íŠ¹ì„± ì¶”ì¶œ: TF-IDF (max_features=10,000)")
print(f"\nğŸ“ˆ ì„±ëŠ¥:")
print(f"  - í•™ìŠµ ë°ì´í„° ì •í™•ë„: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"  - êµì°¨ ê²€ì¦ í‰ê· : {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)")

print(f"\nğŸ’¾ ì €ì¥ëœ íŒŒì¼:")
print(f"  - tfidf_vectorizer.pkl (TF-IDF ë²¡í„°ë¼ì´ì €)")
print(f"  - sentiment_model.pkl (í•™ìŠµëœ ëª¨ë¸)")
print(f"  - 03_confusion_matrix.png (í˜¼ë™ í–‰ë ¬)")
print(f"  - 04_feature_importance.png (íŠ¹ì„± ì¤‘ìš”ë„)")

print("\n" + "="*70)
print("ğŸ‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("ë‹¤ìŒ ë‹¨ê³„: 03_predict.pyë¡œ ìƒˆë¡œìš´ ë¦¬ë·° ì˜ˆì¸¡")
print("="*70)

