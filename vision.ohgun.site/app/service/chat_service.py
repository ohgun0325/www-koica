"""
ğŸ˜ğŸ˜ chat_service.py ì„œë¹™ ê´€ë ¨ ì„œë¹„ìŠ¤

ë‹¨ìˆœ ì±„íŒ…/ëŒ€í™”í˜• LLM ì¸í„°í˜ì´ìŠ¤.

ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ìš”ì•½, í† í° ì ˆì•½ ì „ëµ ë“±.
QLoRA ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ëŒ€í™”/í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤.
"""
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel,
    TaskType,
)
from datasets import Dataset

try:
    # Try the correct import path first
    from trl.trainer.sft_trainer import SFTTrainer  # type: ignore
    TRL_AVAILABLE = True
except ImportError:
    # If that fails, try alternative paths (for different trl versions)
    try:
        from trl.trainer import SFTTrainer  # type: ignore
        TRL_AVAILABLE = True
    except ImportError:
        try:
            from trl import SFTTrainer  # type: ignore
            TRL_AVAILABLE = True
        except ImportError:
            TRL_AVAILABLE = False
            SFTTrainer = None  # type: ignore


class QLoRAChatService:
    """QLoRAë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ë° í•™ìŠµ ì„œë¹„ìŠ¤."""

    def __init__(
        self,
        model_name: str = "K-intelligence/Midm-2.0-Mini-Instruct",
        output_dir: str = "models/qlora_checkpoints",
        use_4bit: bool = True,
        bnb_4bit_compute_dtype: str = "float16",
        bnb_4bit_quant_type: str = "nf4",
        bnb_4bit_use_double_quant: bool = True,
        device_map: str = "auto",
    ):
        """QLoRA ì±„íŒ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™”.

        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
            output_dir: í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            use_4bit: 4-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
            bnb_4bit_compute_dtype: ê³„ì‚° ë°ì´í„° íƒ€ì…
            bnb_4bit_quant_type: ì–‘ìí™” íƒ€ì…
            bnb_4bit_use_double_quant: ì´ì¤‘ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
            device_map: ë””ë°”ì´ìŠ¤ ë§¤í•‘ ì „ëµ
        """
        self.model_name = model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device_map = device_map

        # BitsAndBytes ì„¤ì • (ì˜¬ë°”ë¥¸ íŒ¨í„´)
        self.bnb_config = None
        if use_4bit and torch.cuda.is_available():
            self.bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type=bnb_4bit_quant_type,
                bnb_4bit_compute_dtype=torch.float16,  # torch dtype ì§ì ‘ ì‚¬ìš©
            )
            print("âœ… 4-bit ì–‘ìí™” ì„¤ì • ì™„ë£Œ")

        self.model = None
        self.tokenizer = None
        self.peft_model = None
        self.is_loaded = False

    def load_model(self) -> None:
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³  QLoRAë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
        if self.is_loaded:
            print("âš ï¸  ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="right",
        )

        # pad_token ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # ëª¨ë¸ ë¡œë“œ (ì˜¬ë°”ë¥¸ íŒ¨í„´ ì ìš©)
        if self.bnb_config and torch.cuda.is_available():
            # QLoRA ë°©ì‹: 4-bit ì–‘ìí™” + cuda device_map
            print("ğŸ”§ 4-bit ì–‘ìí™” ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”©...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=self.bnb_config,
                device_map="cuda",
                trust_remote_code=True,
            )
        else:
            # ì¼ë°˜ ëª¨ë“œ
            print("ğŸ”§ ì¼ë°˜ ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”©...")
            model_kwargs: dict = {
                "trust_remote_code": True,
                "device_map": "cuda" if torch.cuda.is_available() else "cpu",
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
            }
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )

        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,  # LoRA rank
            lora_alpha=32,  # LoRA alpha
            lora_dropout=0.1,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Midm ëª¨ë¸ì˜ attention ëª¨ë“ˆ
            bias="none",
        )

        # PEFT ëª¨ë¸ ì ìš© (ì˜¬ë°”ë¥¸ íŒ¨í„´)
        print("ğŸ”§ LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...")
        self.peft_model = get_peft_model(self.model, lora_config)
        self.peft_model.print_trainable_parameters()

        self.is_loaded = True
        print("âœ… QLoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def load_peft_model(self, peft_model_path: str) -> None:
        """í•™ìŠµëœ PEFT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            peft_model_path: PEFT ëª¨ë¸ ê²½ë¡œ
        """
        if not self.is_loaded:
            self.load_model()

        if self.model is None:
            raise RuntimeError("ê¸°ë³¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        print(f"ğŸ“¦ PEFT ëª¨ë¸ ë¡œë”© ì¤‘: {peft_model_path}")
        self.peft_model = PeftModel.from_pretrained(
            self.model,
            peft_model_path,
            device_map=self.device_map,
        )
        print("âœ… PEFT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def chat(
        self,
        message: str,
        history: Optional[List[Dict[str, str]]] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> str:
        """ëŒ€í™”í˜• ì±„íŒ…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒì‚¬í•­)
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            top_p: nucleus sampling íŒŒë¼ë¯¸í„°

        Returns:
            AI ì‘ë‹µ
        """
        if not self.is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        if self.tokenizer is None:
            raise RuntimeError("í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if self.peft_model is None:
            raise RuntimeError("PEFT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # íˆìŠ¤í† ë¦¬ì™€ í˜„ì¬ ë©”ì‹œì§€ë¥¼ í¬ë§·íŒ…
        if history is None:
            history = []

        # Midm instruction í¬ë§·
        formatted_messages = []
        for h in history:
            if h.get("role") == "user":
                formatted_messages.append(f"ì§ˆë¬¸: {h.get('content', '')}")
            elif h.get("role") == "assistant":
                formatted_messages.append(f"ë‹µë³€: {h.get('content', '')}")

        formatted_messages.append(f"ì§ˆë¬¸: {message}")
        formatted_messages.append("ë‹µë³€:")

        prompt = "\n".join(formatted_messages)

        # í† í¬ë‚˜ì´ì§• (token_type_ids ì œì™¸)
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048,
            return_token_type_ids=False,  # Midm ëª¨ë¸ì€ token_type_idsë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        )

        # token_type_idsê°€ ìˆìœ¼ë©´ ì œê±° (ì•ˆì „ì¥ì¹˜)
        if "token_type_ids" in inputs:
            inputs.pop("token_type_ids")

        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        # ìƒì„±
        with torch.no_grad():
            outputs = self.peft_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )

        # í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
        if "ë‹µë³€:" in generated_text:
            response = generated_text.split("ë‹µë³€:")[-1].strip()
        else:
            response = generated_text[len(prompt):].strip()

        return response

    def train(
        self,
        training_data: List[Dict[str, str]],
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 2e-4,
        save_steps: int = 100,
        logging_steps: int = 10,
        output_subdir: Optional[str] = None,
    ) -> str:
        """QLoRA ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

        Args:
            training_data: í•™ìŠµ ë°ì´í„° [{"instruction": "...", "input": "...", "output": "..."}]
            num_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            learning_rate: í•™ìŠµë¥ 
            save_steps: ì €ì¥ ê°„ê²©
            logging_steps: ë¡œê¹… ê°„ê²©
            output_subdir: ì¶œë ¥ ì„œë¸Œë””ë ‰í† ë¦¬

        Returns:
            í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        """
        if not self.is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        if self.tokenizer is None:
            raise RuntimeError("í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if self.peft_model is None:
            raise RuntimeError("PEFT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print(f"ğŸš€ í•™ìŠµ ì‹œì‘: {len(training_data)}ê°œ ìƒ˜í”Œ")

        # ë°ì´í„° í¬ë§·íŒ…
        def format_prompt(example):
            instruction = example.get("instruction", "")
            input_text = example.get("input", "")
            output = example.get("output", "")

            if input_text:
                prompt = f"ì§ˆë¬¸: {instruction}\n{input_text}\në‹µë³€: {output}"
            else:
                prompt = f"ì§ˆë¬¸: {instruction}\në‹µë³€: {output}"

            return {"text": prompt}

        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = Dataset.from_list(training_data)
        dataset = dataset.map(format_prompt)

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_subdir:
            output_path = self.output_dir / output_subdir
        else:
            output_path = self.output_dir / "latest"

        output_path.mkdir(parents=True, exist_ok=True)

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=str(output_path),
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=4,
            learning_rate=learning_rate,
            fp16=torch.cuda.is_available(),
            logging_steps=logging_steps,
            save_steps=save_steps,
            save_total_limit=3,
            optim="paged_adamw_8bit" if self.bnb_config else "adamw_torch",
            warmup_steps=100,
            report_to="none",
        )

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )

        # TRLì˜ SFTTrainer ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        if TRL_AVAILABLE and SFTTrainer is not None:
            # SFTTrainerì˜ íŒŒë¼ë¯¸í„° í™•ì¸ ë° ì‚¬ìš©
            try:
                import inspect
                sig = inspect.signature(SFTTrainer.__init__)
                params = sig.parameters

                # SFTTrainer íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í˜¸ì¶œ
                trainer_kwargs = {
                    "model": self.peft_model,
                    "train_dataset": dataset,
                    "args": training_args,
                }

                # tokenizer íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€ (SFTTrainerëŠ” tokenizerë¥¼ ë°›ìŒ)
                if "tokenizer" in params:
                    trainer_kwargs["tokenizer"] = self.tokenizer
                # TrainerëŠ” tokenizerë¥¼ ì§ì ‘ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ

                # max_seq_length íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if "max_seq_length" in params:
                    trainer_kwargs["max_seq_length"] = 2048

                # data_collator íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if "data_collator" in params:
                    trainer_kwargs["data_collator"] = data_collator

                trainer = SFTTrainer(**trainer_kwargs)
            except Exception as e:
                print(f"âš ï¸  SFTTrainer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, ê¸°ë³¸ Trainer ì‚¬ìš©")
                trainer = Trainer(
                    model=self.peft_model,
                    train_dataset=dataset,
                    args=training_args,
                    data_collator=data_collator,
                )
        else:
            # ê¸°ë³¸ Trainer ì‚¬ìš© (tokenizerëŠ” data_collatorì—ì„œ ì²˜ë¦¬)
            trainer = Trainer(
                model=self.peft_model,
                train_dataset=dataset,
                args=training_args,
                data_collator=data_collator,
            )

        # í•™ìŠµ ì‹¤í–‰
        trainer.train()

        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        self.tokenizer.save_pretrained(str(output_path))

        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_path}")

        return str(output_path)

    def save_model(self, save_path: Optional[str] = None) -> str:
        """í˜„ì¬ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)

        Returns:
            ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
        """
        if not self.is_loaded:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if self.peft_model is None:
            raise RuntimeError("PEFT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if self.tokenizer is None:
            raise RuntimeError("í† í¬ë‚˜ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        if save_path is None:
            save_path = str(self.output_dir / "latest")

        Path(save_path).mkdir(parents=True, exist_ok=True)

        self.peft_model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)

        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        return save_path

    def unload_model(self) -> None:
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•©ë‹ˆë‹¤."""
        if self.model is not None:
            del self.model
            self.model = None

        if self.peft_model is not None:
            del self.peft_model
            self.peft_model = None

        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.is_loaded = False
        print("âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì‚¬í•­)
_global_service: Optional[QLoRAChatService] = None


def get_chat_service(
    model_name: str = "K-intelligence/Midm-2.0-Mini-Instruct",
    **kwargs
) -> QLoRAChatService:
    """ì „ì—­ ì±„íŒ… ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        **kwargs: ì¶”ê°€ ì¸ì

    Returns:
        QLoRAChatService ì¸ìŠ¤í„´ìŠ¤
    """
    global _global_service
    if _global_service is None:
        _global_service = QLoRAChatService(model_name=model_name, **kwargs)
    return _global_service
