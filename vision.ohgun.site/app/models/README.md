# ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ

## ğŸ“ êµ¬ì¡°

```
app/models/
â”œâ”€â”€ __init__.py          # íŒ¨í‚¤ì§€ ì´ˆê¸°í™” ë° ê³µê°œ API
â”œâ”€â”€ base.py              # ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ (BaseLLMModel, BaseEmbeddingModel)
â”œâ”€â”€ loader.py             # ëª¨ë¸ ë¡œë” (ë””ìŠ¤í¬ì—ì„œ ëª¨ë¸ ë¡œë“œ)
â”œâ”€â”€ manager.py            # ëª¨ë¸ ë§¤ë‹ˆì € (ì‹±ê¸€í†¤, ìƒëª…ì£¼ê¸° ê´€ë¦¬)
â””â”€â”€ README.md             # ì´ ë¬¸ì„œ
```

## ğŸ¯ ì„¤ê³„ ì›ì¹™

### 1. ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ ì„¤ê³„
- `BaseLLMModel`: ëª¨ë“  ì±„íŒ… ëª¨ë¸ì˜ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤
- `BaseEmbeddingModel`: ëª¨ë“  ì„ë² ë”© ëª¨ë¸ì˜ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤
- LangChainì˜ `BaseChatModel` ë° `Embeddings`ì™€ í˜¸í™˜

### 2. ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬
- **Loader**: ëª¨ë¸ íŒŒì¼ì„ ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
- **Manager**: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ì˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬ ë° ìºì‹±
- **Base**: ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì •ì˜

### 3. í™•ì¥ ê°€ëŠ¥ì„±
- ìƒˆë¡œìš´ ëª¨ë¸ í˜•ì‹ ì¶”ê°€ ì‹œ `BaseLLMModel` êµ¬í˜„ë§Œ í•˜ë©´ ë¨
- HuggingFace, ONNX, TensorFlow ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì› ê°€ëŠ¥

## ğŸ“¦ ëª¨ë¸ ì €ì¥ì†Œ êµ¬ì¡°

```
models/                          # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ models/ ë””ë ‰í† ë¦¬
â”œâ”€â”€ model-name-1/               # ê° ëª¨ë¸ì€ ë³„ë„ ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ model.safetensors       # ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ tokenizer.json          # í† í¬ë‚˜ì´ì €
â”‚   â”œâ”€â”€ tokenizer_config.json   # í† í¬ë‚˜ì´ì € ì„¤ì •
â”‚   â”œâ”€â”€ config.json             # ëª¨ë¸ ì„¤ì •
â”‚   â”œâ”€â”€ generation_config.json  # ìƒì„± ì„¤ì •
â”‚   â””â”€â”€ ...
â””â”€â”€ model-name-2/
    â””â”€â”€ ...
```

## ğŸ”§ ì‚¬ìš© ë°©ë²•

### 1. ëª¨ë¸ êµ¬í˜„ ì˜ˆì‹œ

```python
# app/models/huggingface.py (ì˜ˆì‹œ)
from typing import List, Any
from app.models.base import BaseLLMModel
from langchain_core.messages import BaseMessage

class HuggingFaceLLM(BaseLLMModel):
    """HuggingFace ëª¨ë¸ êµ¬í˜„ ì˜ˆì‹œ."""

    def load(self) -> None:
        from transformers import AutoModelForCausalLM, AutoTokenizer

        self._tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self._model = AutoModelForCausalLM.from_pretrained(self.model_path)
        self._is_loaded = True

    def unload(self) -> None:
        del self._model
        del self._tokenizer
        self._model = None
        self._tokenizer = None
        self._is_loaded = False

    def invoke(self, messages: List[Any]) -> Any:
        # ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text = self._format_messages(messages)

        # í† í¬ë‚˜ì´ì§•
        inputs = self._tokenizer(text, return_tensors="pt")

        # ìƒì„±
        outputs = self._model.generate(**inputs)

        # ë””ì½”ë”©
        response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)

        return response
```

### 2. ëª¨ë¸ ë§¤ë‹ˆì € ì‚¬ìš©

```python
from app.models.manager import ModelManager

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
manager = ModelManager()

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸
models = manager.list_available_models()
print(f"Available models: {models}")

# ì±„íŒ… ëª¨ë¸ ë¡œë“œ
chat_model = manager.get_chat_model("my-model-name")

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = manager.get_embedding_model("my-embedding-model")

# ëª¨ë¸ ì–¸ë¡œë“œ
manager.unload_all()
```

### 3. ê¸°ì¡´ ì½”ë“œì™€ í†µí•©

```python
# app/core/chat_chain.py ìˆ˜ì • ì˜ˆì‹œ
from app.models.manager import ModelManager
from app.models.base import BaseLLMModel

def chat_with_ai(
    conn,
    user_input: str,
    dimension: int,
    chat_model: Optional[BaseLLMModel] = None  # BaseLLMModel ì‚¬ìš©
) -> str:
    # ëª¨ë¸ ë§¤ë‹ˆì €ì—ì„œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    if chat_model is None:
        manager = ModelManager()
        chat_model = manager.get_chat_model()

    # ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©
    # chat_model.invoke([system_message, human_message])
    ...
```

## âš™ï¸ ì„¤ì •

`config.py`ì—ì„œ ëª¨ë¸ ê´€ë ¨ ì„¤ì •:

```python
# .env íŒŒì¼
LOCAL_MODELS_DIR=models
DEFAULT_CHAT_MODEL=my-chat-model
DEFAULT_EMBEDDING_MODEL=my-embedding-model
MODEL_DEVICE=cpu  # ë˜ëŠ” cuda, mps
MODEL_DTYPE=float32  # ë˜ëŠ” float16, bfloat16
```

## ğŸ”„ ëª¨ë¸ ì£¼ì… íë¦„

```
1. ëª¨ë¸ íŒŒì¼ ì¤€ë¹„
   â””â”€â”€ models/my-model/ ë””ë ‰í† ë¦¬ì— ëª¨ë¸ íŒŒì¼ ë³µì‚¬

2. ëª¨ë¸ í´ë˜ìŠ¤ êµ¬í˜„
   â””â”€â”€ BaseLLMModel ë˜ëŠ” BaseEmbeddingModel ìƒì†

3. ModelManagerì— ë“±ë¡
   â””â”€â”€ manager.get_chat_model("my-model", MyModelClass)

4. ì‚¬ìš©
   â””â”€â”€ chat_chain.pyì—ì„œ ëª¨ë¸ ì‚¬ìš©
```

## ğŸ“ TODO

- [ ] HuggingFace ëª¨ë¸ êµ¬í˜„ ì˜ˆì‹œ ì¶”ê°€
- [ ] ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒ¨í„´ êµ¬í˜„ (ì„ íƒì )
- [ ] ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- [ ] ëª¨ë¸ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- [ ] ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## ğŸ“ ì°¸ê³ 

- [LangChain Model Integration](https://python.langchain.com/docs/modules/model_io/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)
- [Model Loading Best Practices](https://huggingface.co/docs/transformers/main/en/model_sharing)

